{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade gensim\n",
    "#pip install spacy==2.1.0\n",
    "#pip install neuralcoref\n",
    "#pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jason13nn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jason13nn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences \n",
    "import neuralcoref\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, porter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "#To fix random output\n",
    "np.random.seed(2018)\n",
    "\n",
    "#Compute execution time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus: 20-Newsgroups dataset \n",
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "df.head()\n",
    "\n",
    "#stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Creating Bigram\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# Remove Stopwords, Make Bigrams and Lemmatize\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Do lemmatization keeping only noun\n",
    "data_lemmatized_noun = lemmatization(data_words_bigrams, allowed_postags=['NOUN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Model with noun only vs. only noun, adj, vb, adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "id2word_noun = corpora.Dictionary(data_lemmatized_noun)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "texts_noun = data_lemmatized_noun\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "corpus_noun = [id2word_noun.doc2bow(text) for text in texts_noun]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:03:23.739249\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.090*\"team\" + 0.089*\"game\" + 0.046*\"play\" + 0.046*\"year\" + 0.028*\"season\" + 0.027*\"fan\" + 0.021*\"nhl\" + 0.018*\"division\" + 0.017*\"boston\" + 0.015*\"lose\"'), (1, '0.141*\"space\" + 0.038*\"earth\" + 0.035*\"launch\" + 0.031*\"mission\" + 0.030*\"orbit\" + 0.029*\"nasa\" + 0.025*\"satellite\" + 0.022*\"mar\" + 0.022*\"plane\" + 0.016*\"map\"'), (2, '0.072*\"drive\" + 0.046*\"card\" + 0.041*\"mac\" + 0.036*\"driver\" + 0.030*\"cpu\" + 0.026*\"memory\" + 0.022*\"scsi\" + 0.021*\"chip\" + 0.021*\"machine\" + 0.020*\"device\"'), (3, '0.037*\"armenian\" + 0.031*\"soldier\" + 0.029*\"greek\" + 0.027*\"village\" + 0.025*\"jew\" + 0.023*\"turk\" + 0.021*\"muslim\" + 0.019*\"turkish\" + 0.016*\"occupy\" + 0.016*\"jewish\"'), (4, '0.114*\"image\" + 0.077*\"scan\" + 0.063*\"format\" + 0.035*\"brian\" + 0.031*\"nec\" + 0.019*\"specification\" + 0.019*\"finland\" + 0.016*\"hewlett_packard\" + 0.014*\"pd\" + 0.010*\"timing\"'), (5, '0.067*\"line\" + 0.064*\"subject\" + 0.061*\"organization\" + 0.035*\"write\" + 0.030*\"article\" + 0.025*\"university\" + 0.023*\"host\" + 0.017*\"get\" + 0.015*\"would\" + 0.015*\"reply\"'), (6, '0.141*\"win\" + 0.049*\"score\" + 0.046*\"notice\" + 0.042*\"run\" + 0.038*\"corp\" + 0.026*\"stat\" + 0.019*\"pitch\" + 0.019*\"super\" + 0.018*\"frame\" + 0.016*\"william\"'), (7, '0.038*\"not\" + 0.023*\"do\" + 0.022*\"would\" + 0.016*\"say\" + 0.014*\"be\" + 0.014*\"people\" + 0.014*\"think\" + 0.013*\"make\" + 0.013*\"go\" + 0.011*\"know\"'), (8, '0.051*\"program\" + 0.050*\"window\" + 0.047*\"file\" + 0.023*\"use\" + 0.020*\"entry\" + 0.019*\"software\" + 0.018*\"run\" + 0.017*\"application\" + 0.017*\"user\" + 0.016*\"available\"'), (9, '0.033*\"use\" + 0.017*\"may\" + 0.015*\"also\" + 0.013*\"number\" + 0.011*\"include\" + 0.010*\"provide\" + 0.010*\"information\" + 0.008*\"source\" + 0.008*\"group\" + 0.008*\"follow\"'), (10, '0.120*\"god\" + 0.059*\"christian\" + 0.036*\"faith\" + 0.033*\"believe\" + 0.030*\"bible\" + 0.027*\"religion\" + 0.024*\"belief\" + 0.024*\"atheist\" + 0.022*\"church\" + 0.021*\"reason\"'), (11, '0.065*\"disk\" + 0.057*\"master\" + 0.045*\"bus\" + 0.028*\"cable\" + 0.025*\"pa\" + 0.024*\"pat\" + 0.023*\"edu\" + 0.021*\"crash\" + 0.020*\"case_western\" + 0.020*\"reserve_university\"'), (12, '0.096*\"key\" + 0.028*\"public\" + 0.027*\"encryption\" + 0.026*\"chip\" + 0.025*\"government\" + 0.024*\"security\" + 0.021*\"clipper\" + 0.020*\"tap\" + 0.019*\"use\" + 0.016*\"secure\"'), (13, '0.050*\"moon\" + 0.035*\"door\" + 0.029*\"motto\" + 0.024*\"california_institute\" + 0.023*\"technology_pasadena\" + 0.022*\"prize\" + 0.022*\"keith\" + 0.021*\"production\" + 0.019*\"uunet\" + 0.019*\"spacecraft\"'), (14, '0.150*\"law\" + 0.067*\"society\" + 0.046*\"moral\" + 0.031*\"homosexual\" + 0.029*\"constitution\" + 0.027*\"morality\" + 0.026*\"ethnic\" + 0.009*\"political_atheist\" + 0.008*\"act\" + 0.007*\"clause\"'), (15, '0.021*\"state\" + 0.016*\"israel\" + 0.016*\"gun\" + 0.014*\"child\" + 0.013*\"man\" + 0.012*\"right\" + 0.012*\"kill\" + 0.011*\"death\" + 0.011*\"physical\" + 0.011*\"day\"'), (16, '0.024*\"car\" + 0.017*\"high\" + 0.015*\"power\" + 0.013*\"cost\" + 0.012*\"low\" + 0.011*\"new\" + 0.010*\"drive\" + 0.010*\"price\" + 0.009*\"rate\" + 0.009*\"model\"'), (17, '0.067*\"mouse\" + 0.049*\"mhz\" + 0.035*\"procedure\" + 0.029*\"modem\" + 0.028*\"band\" + 0.023*\"default\" + 0.023*\"receiver\" + 0.020*\"edit\" + 0.020*\"terminal\" + 0.017*\"forsale\"'), (18, '0.137*\"video\" + 0.028*\"nt\" + 0.014*\"sue\" + 0.009*\"lt\" + 0.008*\"australian\" + 0.006*\"ini\" + 0.004*\"image_process\" + 0.002*\"canberra\" + 0.000*\"svein_pedersen\" + 0.000*\"svein\"'), (19, '0.620*\"ax\" + 0.063*\"_\" + 0.046*\"max\" + 0.018*\"pin\" + 0.010*\"upgrade\" + 0.009*\"dod\" + 0.009*\"gateway\" + 0.007*\"rider\" + 0.007*\"brown\" + 0.006*\"blind\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0 is a represented as '0.048*\"peter\" + 0.036*\"ice\" + 0.024*\"spring\" + 0.022*\"last_night\" + 0.020*\"minnesota\" + 0.019*\"speaker\" + 0.017*\"backup\" + 0.017*\"custom\" + 0.013*\"significance\" + 0.011*\"tour\"'.\n",
    "\n",
    "It means the top 10 keywords that contribute to this topic are: ‘peter’, ‘ice’, ‘spring’.. and so on and the weight of ‘peter’ on topic 0 is 0.048.\n",
    "\n",
    "The weights reflect how important a keyword is to that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with noun only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:02:23.717327\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "lda_model_noun = gensim.models.ldamodel.LdaModel(corpus=corpus_noun,\n",
    "                                           id2word=id2word_noun,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.090*\"team\" + 0.089*\"game\" + 0.046*\"play\" + 0.046*\"year\" + 0.028*\"season\" + 0.027*\"fan\" + 0.021*\"nhl\" + 0.018*\"division\" + 0.017*\"boston\" + 0.015*\"lose\"'), (1, '0.141*\"space\" + 0.038*\"earth\" + 0.035*\"launch\" + 0.031*\"mission\" + 0.030*\"orbit\" + 0.029*\"nasa\" + 0.025*\"satellite\" + 0.022*\"mar\" + 0.022*\"plane\" + 0.016*\"map\"'), (2, '0.072*\"drive\" + 0.046*\"card\" + 0.041*\"mac\" + 0.036*\"driver\" + 0.030*\"cpu\" + 0.026*\"memory\" + 0.022*\"scsi\" + 0.021*\"chip\" + 0.021*\"machine\" + 0.020*\"device\"'), (3, '0.037*\"armenian\" + 0.031*\"soldier\" + 0.029*\"greek\" + 0.027*\"village\" + 0.025*\"jew\" + 0.023*\"turk\" + 0.021*\"muslim\" + 0.019*\"turkish\" + 0.016*\"occupy\" + 0.016*\"jewish\"'), (4, '0.114*\"image\" + 0.077*\"scan\" + 0.063*\"format\" + 0.035*\"brian\" + 0.031*\"nec\" + 0.019*\"specification\" + 0.019*\"finland\" + 0.016*\"hewlett_packard\" + 0.014*\"pd\" + 0.010*\"timing\"'), (5, '0.067*\"line\" + 0.064*\"subject\" + 0.061*\"organization\" + 0.035*\"write\" + 0.030*\"article\" + 0.025*\"university\" + 0.023*\"host\" + 0.017*\"get\" + 0.015*\"would\" + 0.015*\"reply\"'), (6, '0.141*\"win\" + 0.049*\"score\" + 0.046*\"notice\" + 0.042*\"run\" + 0.038*\"corp\" + 0.026*\"stat\" + 0.019*\"pitch\" + 0.019*\"super\" + 0.018*\"frame\" + 0.016*\"william\"'), (7, '0.038*\"not\" + 0.023*\"do\" + 0.022*\"would\" + 0.016*\"say\" + 0.014*\"be\" + 0.014*\"people\" + 0.014*\"think\" + 0.013*\"make\" + 0.013*\"go\" + 0.011*\"know\"'), (8, '0.051*\"program\" + 0.050*\"window\" + 0.047*\"file\" + 0.023*\"use\" + 0.020*\"entry\" + 0.019*\"software\" + 0.018*\"run\" + 0.017*\"application\" + 0.017*\"user\" + 0.016*\"available\"'), (9, '0.033*\"use\" + 0.017*\"may\" + 0.015*\"also\" + 0.013*\"number\" + 0.011*\"include\" + 0.010*\"provide\" + 0.010*\"information\" + 0.008*\"source\" + 0.008*\"group\" + 0.008*\"follow\"'), (10, '0.120*\"god\" + 0.059*\"christian\" + 0.036*\"faith\" + 0.033*\"believe\" + 0.030*\"bible\" + 0.027*\"religion\" + 0.024*\"belief\" + 0.024*\"atheist\" + 0.022*\"church\" + 0.021*\"reason\"'), (11, '0.065*\"disk\" + 0.057*\"master\" + 0.045*\"bus\" + 0.028*\"cable\" + 0.025*\"pa\" + 0.024*\"pat\" + 0.023*\"edu\" + 0.021*\"crash\" + 0.020*\"case_western\" + 0.020*\"reserve_university\"'), (12, '0.096*\"key\" + 0.028*\"public\" + 0.027*\"encryption\" + 0.026*\"chip\" + 0.025*\"government\" + 0.024*\"security\" + 0.021*\"clipper\" + 0.020*\"tap\" + 0.019*\"use\" + 0.016*\"secure\"'), (13, '0.050*\"moon\" + 0.035*\"door\" + 0.029*\"motto\" + 0.024*\"california_institute\" + 0.023*\"technology_pasadena\" + 0.022*\"prize\" + 0.022*\"keith\" + 0.021*\"production\" + 0.019*\"uunet\" + 0.019*\"spacecraft\"'), (14, '0.150*\"law\" + 0.067*\"society\" + 0.046*\"moral\" + 0.031*\"homosexual\" + 0.029*\"constitution\" + 0.027*\"morality\" + 0.026*\"ethnic\" + 0.009*\"political_atheist\" + 0.008*\"act\" + 0.007*\"clause\"'), (15, '0.021*\"state\" + 0.016*\"israel\" + 0.016*\"gun\" + 0.014*\"child\" + 0.013*\"man\" + 0.012*\"right\" + 0.012*\"kill\" + 0.011*\"death\" + 0.011*\"physical\" + 0.011*\"day\"'), (16, '0.024*\"car\" + 0.017*\"high\" + 0.015*\"power\" + 0.013*\"cost\" + 0.012*\"low\" + 0.011*\"new\" + 0.010*\"drive\" + 0.010*\"price\" + 0.009*\"rate\" + 0.009*\"model\"'), (17, '0.067*\"mouse\" + 0.049*\"mhz\" + 0.035*\"procedure\" + 0.029*\"modem\" + 0.028*\"band\" + 0.023*\"default\" + 0.023*\"receiver\" + 0.020*\"edit\" + 0.020*\"terminal\" + 0.017*\"forsale\"'), (18, '0.137*\"video\" + 0.028*\"nt\" + 0.014*\"sue\" + 0.009*\"lt\" + 0.008*\"australian\" + 0.006*\"ini\" + 0.004*\"image_process\" + 0.002*\"canberra\" + 0.000*\"svein_pedersen\" + 0.000*\"svein\"'), (19, '0.620*\"ax\" + 0.063*\"_\" + 0.046*\"max\" + 0.018*\"pin\" + 0.010*\"upgrade\" + 0.009*\"dod\" + 0.009*\"gateway\" + 0.007*\"rider\" + 0.007*\"brown\" + 0.006*\"blind\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score for LDA with all:  0.5431104817051807\n",
      "\n",
      "Coherence Score for LDA with noun only:  0.501404199685498\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "coherence_model_lda_noun = CoherenceModel(model=lda_model_noun, texts=data_lemmatized_noun, dictionary=id2word_noun, coherence='c_v')\n",
    "coherence_lda_noun = coherence_model_lda_noun.get_coherence()\n",
    "\n",
    "print('\\nCoherence Score for LDA with all: ', coherence_lda)\n",
    "print('\\nCoherence Score for LDA with noun only: ', coherence_lda_noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have the difference of coherence score between two models are small.\n",
    "Therefore, we can use the model with noun only because it greatly reduce the execution time. (16,004,176 to 8,940,795)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data preprocessing\n",
    "# Neural Coreference\n",
    "nlp = en_core_web_sm.load()\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "nlp.max_length = 1000000\n",
    "\n",
    "data_nc = ' '.join(map(str, data))\n",
    "data_nc = data_nc[:100000]\n",
    "data_nc = nlp(data_nc)\n",
    "data_nc = nlp(data_nc._.coref_resolved)\n",
    "\n",
    "#Remove punctuations\n",
    "data_words_nc = list(sent_to_words(data_nc))\n",
    "\n",
    "# Creating Bigram\n",
    "bigram_nc = gensim.models.Phrases(data_words_nc, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "bigram_mod_nc = gensim.models.phrases.Phraser(bigram_nc)\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nc_nostops = remove_stopwords(data_words_nc)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_nc_bigrams = make_bigrams(data_words_nc_nostops)\n",
    "\n",
    "# Do lemmatization keeping only noun\n",
    "data_nc_lemmatized_noun = lemmatization(data_words_nc_bigrams, allowed_postags=['NOUN'])\n",
    "data_nc_lemmatized_noun = [x for x in data_nc_lemmatized_noun if x] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model with noun only using neural coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_nc_noun = corpora.Dictionary(data_nc_lemmatized_noun)\n",
    "\n",
    "# Create Corpus\n",
    "texts_nc_noun = data_nc_lemmatized_noun\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_nc_noun = [id2word_nc_noun.doc2bow(text) for text in texts_nc_noun]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_nc_noun = gensim.models.ldamodel.LdaModel(corpus=corpus_nc_noun,\n",
    "                                           id2word=id2word_nc_noun,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score for LDA with noun only using neural coref:  0.8218067004115035\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda_nc_noun = CoherenceModel(model=lda_model_nc_noun, texts=data_nc_lemmatized_noun, dictionary=id2word_nc_noun, coherence='c_v')\n",
    "coherence_lda_nc_noun = coherence_model_lda_nc_noun.get_coherence()\n",
    "\n",
    "print('\\nCoherence Score for LDA with noun only using neural coref: ', coherence_lda_nc_noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on coherence score, we can see the model improved using neural coref."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
